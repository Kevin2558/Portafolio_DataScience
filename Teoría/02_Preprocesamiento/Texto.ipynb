{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFhXD4CEhPGdmCh8EEtuCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevin2558/Data_Science/blob/main/02_Preprocesamiento/Texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento de Texto**"
      ],
      "metadata": {
        "id": "ylXNekH4l-on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import re"
      ],
      "metadata": {
        "id": "aud_QhuPl8on"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de aplicar técnicas de modelado de tópicos, es fundamental preparar el texto para que los algoritmos trabajen con datos limpios y consistentes. El preprocesamiento típico incluye varios pasos:\n",
        "\n",
        "1. **Conversión a minúsculas**\n",
        "\n",
        "  Convierte todo el texto a minúsculas para evitar que palabras iguales en diferentes formatos (ej. \"Apple\" y \"apple\") se traten como distintas.\n",
        "\n",
        "2. **Eliminación de números**\n",
        "\n",
        "  Se eliminan dígitos numéricos, que normalmente no aportan significado semántico para los temas, a menos que sean relevantes para el análisis.\n",
        "\n",
        "3. **Eliminación de signos de puntuación y caracteres especiales**\n",
        "  \n",
        "  Se remueven símbolos como comas, puntos, signos de interrogación, etc., que no contribuyen al significado del texto.\n",
        "\n",
        "4. **Tokenización**\n",
        "\n",
        "  Se divide el texto en palabras o tokens individuales para analizarlos por separado.\n",
        "\n",
        "5. **Eliminación de stopwords**\n",
        "\n",
        "  Las stopwords son palabras muy comunes (como \"the\", \"and\", \"is\" en inglés; como \"él\", \"y\", \"es\" en español) que no aportan información significativa sobre los temas.\n",
        "\n",
        "6. **Lematización**\n",
        "\n",
        "  La lematización reduce las palabras a su forma base o raíz (por ejemplo, “running”, “ran” → “run”; por ejemplo, \"panadería\" → \"pan\"), para tratar variantes de la misma palabra como una sola entidad."
      ],
      "metadata": {
        "id": "WD1iQqhLkv9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos NLTK (una sola vez)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "B1GxPaU3nHqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gluT8qxukvHk",
        "outputId": "075a9d6a-be5a-431c-a6a1-874f0de2cef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['advancing' 'ai' 'collection' 'computer' 'document' 'enables' 'field'\n",
            " 'find' 'help' 'hidden' 'human' 'language' 'learning' 'machine' 'modeling'\n",
            " 'natural' 'processing' 'rapidly' 'theme' 'topic' 'understand']\n",
            "\n",
            "Matriz documento-término:\n",
            "[[0 0 0 1 0 1 0 0 0 0 1 2 0 0 0 1 1 0 0 0 1]\n",
            " [0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0]\n",
            " [1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Dataset ejemplo: colección de textos (puede ser un csv con reviews, noticias, etc.)\n",
        "texts = [\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Topic modeling helps find hidden themes in a collection of documents.\",\n",
        "    \"Machine learning and AI are rapidly advancing fields.\"\n",
        "]\n",
        "\n",
        "# Funciones limpieza y lematización\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # quitar números\n",
        "    text = re.sub(r'\\W+', ' ', text) # quitar signos de puntuación\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "texts_clean = [preprocess_text(t) for t in texts]\n",
        "\n",
        "# Vectorización\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts_clean)\n",
        "print(\"Features:\", vectorizer.get_feature_names_out())\n",
        "print()\n",
        "print(\"Matriz documento-término:\")\n",
        "print(X.toarray())"
      ]
    }
  ]
}